{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7Ujz-dsw1pA"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers tqdm matplotlib numpy pandas torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02RHqwEblH7e"
      },
      "outputs": [],
      "source": [
        "!apt install tree -y\n",
        "!rm -rf ./data\n",
        "!mkdir data\n",
        "!cp /content/txt2openpose-Data.zip ./file.zip\n",
        "!unzip ./file.zip -d ./data\n",
        "!clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhKL77L1m0ng"
      },
      "outputs": [],
      "source": [
        "!tree \"/content/data/txt2openpose-Data - Copy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Find bad data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def list_files(root_dir):\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            yield os.path.join(dirpath, filename)\n",
        "\n",
        "folder_path = '/content/data/txt2openpose-Data - Copy'\n",
        "for file_path in list_files(folder_path):\n",
        "  with open(file_path) as f:\n",
        "    data = json.load(f)\n",
        "    if (data[\"canvas_width\"] != 900 or data[\"canvas_height\"] != 300 or len(data[\"people\"]) != 5 ): print(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ph3JBdLnkgB"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN7HdI6vnpvj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import T5Tokenizer\n",
        "import torch.nn as nn\n",
        "from transformers import T5Model\n",
        "\n",
        "# Dataset Class\n",
        "class MotionDataset(Dataset):\n",
        "    def __init__(self, root_dir, tokenizer, max_length=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.file_paths = self._get_file_paths()\n",
        "\n",
        "    def _get_file_paths(self):\n",
        "        file_paths = []\n",
        "        for root, _, files in os.walk(self.root_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.json'):\n",
        "                    file_paths.append(os.path.join(root, file))\n",
        "        return file_paths\n",
        "\n",
        "    def _load_json(self, file_path):\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data\n",
        "\n",
        "    def _extract_keypoints(self, data):\n",
        "        keypoints = []\n",
        "        for person in data['people']:\n",
        "            keypoints.extend(person['pose_keypoints_2d'])\n",
        "        return keypoints\n",
        "\n",
        "    def _extract_path_info(self, file_path):\n",
        "        relative_path = os.path.relpath(file_path, self.root_dir)\n",
        "        parts = relative_path.split(os.sep)\n",
        "        category = parts[0]\n",
        "        subcategory = parts[1]\n",
        "        filename = os.path.splitext(parts[2])[0]\n",
        "        path_info = f\"{category}, {subcategory}, {filename}\"\n",
        "        return path_info\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.file_paths[idx]\n",
        "        data = self._load_json(file_path)\n",
        "        keypoints = self._extract_keypoints(data)\n",
        "\n",
        "        # Reshape keypoints to (num_joints, 3) and then to (num_joints, 2) since z is always 1\n",
        "        keypoints = torch.tensor(keypoints).view(-1, 3)\n",
        "        keypoints = keypoints[:, :2]\n",
        "\n",
        "        path_info = self._extract_path_info(file_path)\n",
        "        encoded_input = self.tokenizer(\n",
        "            path_info,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_input['input_ids'].squeeze()  # [max_length]\n",
        "        attention_mask = encoded_input['attention_mask'].squeeze()  # [max_length]\n",
        "        return input_ids, attention_mask, keypoints\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids, attention_masks, labels = zip(*batch)\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    labels = torch.stack(labels)\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "dataset = MotionDataset('/content/data/txt2openpose-Data - Copy', tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Example to decode batch of input_ids into text\n",
        "for batch in dataloader:\n",
        "    input_ids, attention_masks, keypoints = batch\n",
        "    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
        "    print(decoded_texts)  # List of decoded texts for the batch\n",
        "    print(keypoints.shape)\n",
        "    break  # Remove this break to iterate over the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d2DCwkYxKun"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def plot_fromPerson(person, person_idx):\n",
        "        keypoints = person\n",
        "        keypoints = np.array(keypoints).reshape(-1, 2)\n",
        "\n",
        "        # Plot keypoints\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=10, c='r')\n",
        "\n",
        "        # Connect keypoints\n",
        "        for i, j in [(0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),\n",
        "                     (8, 9), (9, 10), (1, 11), (11, 12), (12, 13)]:\n",
        "            plt.plot([keypoints[i, 0], keypoints[j, 0]],\n",
        "                     [keypoints[i, 1], keypoints[j, 1]], 'r')\n",
        "\n",
        "        # Add label for each person\n",
        "        plt.text(keypoints[0, 0], keypoints[0, 1], f'Person {person_idx}', fontsize=10, color='blue')\n",
        "\n",
        "def plot_openpose(people):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.zeros((300, 900, 3)))  # Create an empty image to plot keypoints on\n",
        "\n",
        "    for idx, person in enumerate(people):\n",
        "      plot_fromPerson(person, idx)\n",
        "\n",
        "    plt.gca()  # Invert y-axis to match image coordinate system\n",
        "    plt.show()\n",
        "\n",
        "def format_keypoints(keypoints):\n",
        "    return keypoints.flatten().reshape(5, 36)\n",
        "\n",
        "for batch in dataloader:\n",
        "    input_ids, attention_masks, keypoints = batch\n",
        "    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
        "    print(decoded_texts)  # List of decoded texts for the batch\n",
        "    print(keypoints.shape)\n",
        "    kp = format_keypoints(keypoints)\n",
        "    plot_openpose(kp)\n",
        "    break  # Remove this break to iterate over the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwxKmWKAlC7n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Model, T5Tokenizer\n",
        "from tqdm import tqdm\n",
        "from torchmetrics.regression import MeanAbsoluteError\n",
        "import math\n",
        "\n",
        "def eval(preds, target):\n",
        "  mean_absolute_error = MeanAbsoluteError()\n",
        "  error = mean_absolute_error(preds, target)\n",
        "  return eval\n",
        "\n",
        "class Text2Motion(nn.Module):\n",
        "    def __init__(self, t5_model_name='t5-small', output_points=90): # total output points (1 dim)\n",
        "        super(Text2Motion, self).__init__()\n",
        "        self.output_points = output_points\n",
        "\n",
        "        # Load T5 model's encoder\n",
        "        self.t5_encoder = T5Model.from_pretrained(t5_model_name).encoder\n",
        "\n",
        "        # Define custom output layer\n",
        "        self.output_layer = nn.Linear(self.t5_encoder.config.hidden_size, self.output_points * 2) # treadted as 2D point\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # Get T5 encoder outputs\n",
        "        encoder_outputs = self.t5_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = encoder_outputs.last_hidden_state  # [batch_size, seq_len, d_model]\n",
        "\n",
        "        # Take the first token's output as the representation (similar to using [CLS] token in BERT)\n",
        "        cls_token_state = hidden_state[:, 0, :]  # [batch_size, d_model]\n",
        "\n",
        "        # Apply custom output layer\n",
        "        motion_output = self.output_layer(cls_token_state)  # [batch_size, output_points * 2]\n",
        "\n",
        "        # Reshape to [batch_size, output_points, 2]\n",
        "        motion_output = motion_output.view(-1, self.output_points, 2)\n",
        "\n",
        "        return motion_output\n",
        "\n",
        "\n",
        "\n",
        "def display_batch_keypoints(list_o_input_ids, list_o_attention_mask, list_o_keypoint, tokenizer):\n",
        "    decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in input_ids]\n",
        "    for idx, text in enumerate(decoded_texts):\n",
        "      print(text)\n",
        "      plot_openpose(format_keypoints(list_o_keypoint[idx]))\n",
        "\n",
        "loss_logs = []\n",
        "\n",
        "current_epoch = 0\n",
        "# Define a simple training loop\n",
        "def train(model, dataloader, optimizer, criterion, device, tokenizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for idx, batch in enumerate(tqdm(dataloader, desc=\"Steps\")):\n",
        "        input_ids, attention_masks, targets = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_masks)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss = loss.item()\n",
        "        total_loss += loss\n",
        "        loss_logs.append(loss)\n",
        "\n",
        "        if (current_epoch % 50 == 0) and (idx == len(batch) - 1):\n",
        "          display_batch_keypoints(\n",
        "              list_o_input_ids=input_ids,\n",
        "              list_o_attention_mask=attention_masks,\n",
        "              list_o_keypoint = outputs.cpu().detach().numpy(),\n",
        "              tokenizer=tokenizer\n",
        "          )\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Example usage:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "dataset = MotionDataset('/content/data/txt2openpose-Data - Copy', tokenizer)\n",
        "\n",
        "train_size = math.floor(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader  = DataLoader(test_dataset, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model = Text2Motion().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2000\n",
        "for epoch in tqdm(range(num_epochs), desc=\"- Epocs\"):\n",
        "    current_epoch = epoch\n",
        "    loss = train(model, train_dataloader, optimizer, criterion, device, tokenizer)\n",
        "    if (epoch % 50 == 0):\n",
        "        plt.plot(loss_logs)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZidVYDLLkQ2"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), './2000_cpkt.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwsRZVhKHgMg"
      },
      "outputs": [],
      "source": [
        "plt.plot(loss_logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFx3OQbKxYhN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "mae = []\n",
        "MAELoss = nn.L1Loss()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    input_ids, attention_masks, targets = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_masks = attention_masks.to(device)\n",
        "    targets = targets.to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_masks)\n",
        "    test_loss = MAELoss(outputs, targets)\n",
        "    mae.append(test_loss.cpu().detach())\n",
        "\n",
        "print(\"MAE Mean: \", np.array(mae).mean())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
