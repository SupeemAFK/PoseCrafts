{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwiy775JFGhZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geIDWup6ZS_J"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch transformers tqdm matplotlib numpy pandas torchmetrics sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVvKApcy7uRv"
      },
      "source": [
        "# **Find bad data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C90AdvTPcdvS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def list_files(root_dir):\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            yield os.path.join(dirpath, filename)\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/txt2openpose-Data - Copy'\n",
        "for file_path in list_files(folder_path):\n",
        "  with open(file_path) as f:\n",
        "    data = json.load(f)\n",
        "    if (data[\"canvas_width\"] != 900 or data[\"canvas_height\"] != 300 or len(data[\"people\"]) != 5 ): print(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpxL8fS48f6x"
      },
      "source": [
        "# **Count Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x65EIHe8jhW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def count_files(directory):\n",
        "    total_files = 0\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        total_files += len(files)\n",
        "    return total_files\n",
        "\n",
        "def display_tree(directory, indent=0):\n",
        "    if not os.path.isdir(directory):\n",
        "        return\n",
        "\n",
        "    # Display current directory\n",
        "    print(\"|   \" * indent + \"|---\" + os.path.basename(directory) + ((15 - len(os.path.basename(directory))) * \" \"), end=\"\")\n",
        "\n",
        "    # Count files in current directory\n",
        "    file_count = count_files(directory)\n",
        "    print(\" ({0} Sequences)\".format(file_count))\n",
        "\n",
        "    # Display subdirectories recursively\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            display_tree(item_path, indent + 1)\n",
        "\n",
        "# Replace 'path_to_your_directory' with the path to your directory\n",
        "display_tree('/content/drive/MyDrive/txt2openpose-Data - Copy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djvNtjBZXItE"
      },
      "source": [
        "# **Plot to see data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tciNonXb9LxW"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/MyDrive/txt2openpose-Data - Copy'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbddAxQ5oMe0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_fromPerson(person, person_idx):\n",
        "        keypoints = person['pose_keypoints_2d']\n",
        "        keypoints = np.array(keypoints).reshape(-1, 3)\n",
        "\n",
        "        # Plot keypoints\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=10, c='r')\n",
        "\n",
        "        # Connect keypoints\n",
        "        for i, j in [(0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),\n",
        "                     (8, 9), (9, 10), (1, 11), (11, 12), (12, 13)]:\n",
        "            plt.plot([keypoints[i, 0], keypoints[j, 0]],\n",
        "                     [keypoints[i, 1], keypoints[j, 1]], 'r')\n",
        "\n",
        "        # Add label for each person\n",
        "        plt.text(keypoints[0, 0], keypoints[0, 1], f'Person {person_idx}', fontsize=10, color='blue')\n",
        "\n",
        "def plot_openpose(json_file):\n",
        "    with open(json_file) as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.zeros((300, 900, 3)))  # Create an empty image to plot keypoints on\n",
        "\n",
        "    for idx, person in enumerate(data['people']):\n",
        "      plot_fromPerson(person, idx)\n",
        "\n",
        "    plt.title(json_file)\n",
        "    plt.gca()  # Invert y-axis to match image coordinate system\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "json_file = '/content/drive/MyDrive/txt2openpose-Data - Copy/Walk/Forward/White-queen_walk_girl_woman.json'\n",
        "plot_openpose(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhUjGtXZlLHk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "def get_random_file_paths(folder_path, num_files=5):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            file_paths.append(os.path.join(root, file))\n",
        "\n",
        "    random.shuffle(file_paths)\n",
        "    num_files = min(num_files, len(file_paths))\n",
        "    return random.sample(file_paths, num_files)\n",
        "\n",
        "# Example usage:\n",
        "random_file_paths = get_random_file_paths(folder_path, 5)\n",
        "for path in random_file_paths:\n",
        "    plot_openpose(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg7ssLznXX0k"
      },
      "source": [
        "# **Grouped Keypoints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVwJefGatpKS"
      },
      "outputs": [],
      "source": [
        "def groupedKeypointsForPerson(person):\n",
        "  pose_keypoints_2d = person[\"pose_keypoints_2d\"]\n",
        "  grouped_keypoints = []\n",
        "\n",
        "  for i in range(len(pose_keypoints_2d)):\n",
        "    if (i+1) % 3 == 0:\n",
        "      grouped_keypoints.append([pose_keypoints_2d[i-2], pose_keypoints_2d[i-1]])\n",
        "\n",
        "  return grouped_keypoints\n",
        "\n",
        "def groupedKeypointForJSON(json_path):\n",
        "  newData = []\n",
        "  with open(json_path) as f:\n",
        "    data = json.load(f)\n",
        "    for i, person in enumerate(data['people']):\n",
        "      grouped_keypoints = groupedKeypointsForPerson(person)\n",
        "      newData.append(grouped_keypoints)\n",
        "  return newData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-s84fV63fjl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "newPosesData = []\n",
        "\n",
        "def list_files(root_dir):\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            yield os.path.join(dirpath, filename)\n",
        "\n",
        "for file_path in list_files(folder_path):\n",
        "  new_json_keypoints_data = groupedKeypointForJSON(file_path)\n",
        "  newPosesData.append(new_json_keypoints_data)\n",
        "\n",
        "for i, data in enumerate(newPosesData):\n",
        "  print(\"index: {}, length: {}, data: {}\".format(i, len(data), data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2W-f444XieS"
      },
      "source": [
        "# **Create text descirption from file path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcjdAy77wPRy"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "text_description_list = []\n",
        "\n",
        "def list_files(root_dir):\n",
        "    for dirpath, _, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            yield os.path.join(dirpath, filename)\n",
        "\n",
        "for file_path in list_files(folder_path):\n",
        "  split_path = file_path.split('/')\n",
        "  text_description_end_json = split_path[5] + \" \" + split_path[6] + \" \" + split_path[7]\n",
        "  text_description = text_description_end_json[0:len(text_description_end_json) - 5]\n",
        "  text_description_list.append(\" \".join(list(dict.fromkeys(text_description.lower().split(\" \")))))\n",
        "\n",
        "for i, data in enumerate(text_description_list):\n",
        "    print(\"index: {}, length: {}, data: {}\".format(i, len(data), data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTceFEDsX4gE"
      },
      "source": [
        "# **Create text(x) points(y) data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3CqP2lb35ye"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "keypoint_poses_data = np.array(newPosesData).reshape(len(newPosesData), 180).tolist()\n",
        "train_test_data = []\n",
        "for i, text_description in enumerate(text_description_list):\n",
        "  data = [text_description, keypoint_poses_data[i]]\n",
        "  train_test_data.append(data)\n",
        "\n",
        "for data in train_test_data:\n",
        "  print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LRW8kolYDZR"
      },
      "source": [
        "# **Split train test data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiHk7y_83KLv"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cNHAlgeMqcr"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "dataset_size = len(train_test_data)\n",
        "\n",
        "train_size = math.floor(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(train_test_data, [train_size, test_size])\n",
        "\n",
        "print(len(train_data))\n",
        "print(len(test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbTJ0R06YaoM"
      },
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfmV0AvGG293"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orJKW2GB9i66"
      },
      "outputs": [],
      "source": [
        "#Dataloader\n",
        "def collate_batch(batch):\n",
        "    processed_texts = []\n",
        "    processed_poses = []\n",
        "    for text, poses in batch:\n",
        "      processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)\n",
        "      processed_texts.append(processed_text)\n",
        "\n",
        "      poses_output = torch.tensor(poses, dtype=torch.float)\n",
        "      processed_poses.append(poses_output)\n",
        "\n",
        "    processed_texts = torch.stack(processed_texts)\n",
        "    processed_poses = torch.stack(processed_poses)\n",
        "    return processed_texts, processed_poses\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "    train_data, batch_size=1, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, batch_size=1, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKga9SXLi9Mj"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_fromPerson(person, person_idx):\n",
        "        keypoints = person\n",
        "        keypoints = np.array(keypoints).reshape(-1, 2)\n",
        "\n",
        "        # Plot keypoints\n",
        "        plt.scatter(keypoints[:, 0], keypoints[:, 1], s=10, c='r')\n",
        "\n",
        "        # Connect keypoints\n",
        "        for i, j in [(0, 1), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7), (1, 8),\n",
        "                     (8, 9), (9, 10), (1, 11), (11, 12), (12, 13)]:\n",
        "            plt.plot([keypoints[i, 0], keypoints[j, 0]],\n",
        "                     [keypoints[i, 1], keypoints[j, 1]], 'r')\n",
        "\n",
        "        # Add label for each person\n",
        "        plt.text(keypoints[0, 0], keypoints[0, 1], f'Person {person_idx}', fontsize=10, color='blue')\n",
        "\n",
        "def plot_openpose(people):\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.zeros((300, 900, 3)))  # Create an empty image to plot keypoints on\n",
        "\n",
        "    for idx, person in enumerate(people):\n",
        "      plot_fromPerson(person, idx)\n",
        "\n",
        "    plt.gca()  # Invert y-axis to match image coordinate system\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVvXY3tJ_9F9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for i, batch in enumerate(train_dataloader):\n",
        "  print(batch[0].shape, batch[1].shape)\n",
        "  print(\"Batch: \", i+1)\n",
        "  for idx, data in enumerate(batch[1]):\n",
        "    plot_openpose(np.array(data).reshape(5, 36))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTDPiXwZVHWY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScdFNKHsYemp"
      },
      "source": [
        "# **Dense Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GumqRo_-1ssl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim):\n",
        "        super(Dense, self).__init__()\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.o = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, embeddings):\n",
        "        x = self.fc1(embeddings)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.fc5(x)\n",
        "        output = self.o(x)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mk2eamgK0p3A"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 384\n",
        "hidden_dim = 512\n",
        "output_dim = 180\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T6SykU112r9"
      },
      "outputs": [],
      "source": [
        "dense_model = Dense(embedding_dim, hidden_dim, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2zm9FqI2AVp"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(dense_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQz0u-cTk1o3"
      },
      "outputs": [],
      "source": [
        "def get_gradient_norms(model):\n",
        "    total_norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ff0fQSNCEmFh"
      },
      "outputs": [],
      "source": [
        "trainingEpoch_loss = []\n",
        "validationEpoch_loss = []\n",
        "gradient_norms = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    step_loss = []\n",
        "    dense_model.train()\n",
        "    for idx, train_inputs in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = dense_model(train_inputs[0].to(device))\n",
        "        training_loss = criterion(outputs, train_inputs[1].to(device))\n",
        "        training_loss.backward()\n",
        "\n",
        "        grad_norm = get_gradient_norms(dense_model)\n",
        "        gradient_norms.append(grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        step_loss.append(training_loss.item())\n",
        "\n",
        "        if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')\n",
        "    trainingEpoch_loss.append(np.array(step_loss).mean())\n",
        "\n",
        "    #dense_model.eval()\n",
        "    #for idx, val_inputs in enumerate(val_dataloader):\n",
        "    #  validationStep_loss = []\n",
        "    #  outputs = dense_model(val_inputs[0].to(device))\n",
        "    #  val_loss = criterion(outputs, val_inputs[1].to(device))\n",
        "    #  validationStep_loss.append(val_loss.item())\n",
        "    #validationEpoch_loss.append(np.array(validationStep_loss).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJf0H9qjk95V"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gradient_norms)\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Gradient norm')\n",
        "plt.title('Gradient Norms during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_55B9DzV5pN"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(trainingEpoch_loss, label='train_loss')\n",
        "plt.plot(validationEpoch_loss,label='val_loss')\n",
        "plt.legend()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KabWxW8qJNe"
      },
      "outputs": [],
      "source": [
        "#Save model to save weight folder\n",
        "model_save_name = 'dense_5layers_1batch_noval.pt'\n",
        "path = F\"/content/drive/MyDrive/Save Weight/{model_save_name}\"\n",
        "torch.save(dense_model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG1ezdnPqMbt"
      },
      "outputs": [],
      "source": [
        "#Load saved model\n",
        "dense_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Save Weight/dense_5layers_1batch_noval.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWWFfaWUj22y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "dense_model.eval()\n",
        "mae = []\n",
        "MAELoss = nn.L1Loss()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    outputs = dense_model(batch[0].to(device))\n",
        "    test_loss = MAELoss(outputs, batch[1].to(device))\n",
        "    mae.append(test_loss)\n",
        "\n",
        "print(\"MAE Mean: \", np.array(mae).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhBhr2X5MIwC"
      },
      "outputs": [],
      "source": [
        "text = \"jump up\"\n",
        "processed_text = processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)\n",
        "output_poses = dense_model(processed_text.to(device))\n",
        "print(output_poses.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX6Upr6JPZsh"
      },
      "outputs": [],
      "source": [
        "keypoints_data = output_poses.tolist()\n",
        "people = []\n",
        "count = 1\n",
        "for i, xy in enumerate(keypoints_data):\n",
        "  if (i+1) % 36 == 0:\n",
        "    people.append(keypoints_data[i+1-36 : 36*count])\n",
        "    count += 1\n",
        "\n",
        "people = np.array(people).reshape(5 ,36)\n",
        "print(people.shape)\n",
        "for person in people.tolist():\n",
        "  print(person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekJAqFBGYqs6"
      },
      "outputs": [],
      "source": [
        "plot_openpose(people)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtbtpbUVtvW8"
      },
      "source": [
        "# **RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrXs1qa6t72W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(RNN, self).__init__()\n",
        "        self.rnn1 = nn.RNN(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.rnn3 = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.rnn4 = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.rnn5 = nn.RNN(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.o = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        o_n1, h_n1 = self.rnn1(embedding)\n",
        "        o_n2, h_n2 = self.rnn2(o_n1, h_n1)\n",
        "        o_n3, h_n3 = self.rnn3(o_n2, h_n2)\n",
        "        o_n4, h_n4 = self.rnn4(o_n3, h_n3)\n",
        "        o_n5, h_n5 = self.rnn5(o_n4, h_n4)\n",
        "        output = self.o(o_n5)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQRdI7bzuFen"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 384\n",
        "hidden_dim = 512\n",
        "num_layers = 1\n",
        "output_dim = 180\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I30Mn_D_uGnn"
      },
      "outputs": [],
      "source": [
        "rnn_model = RNN(embedding_dim, hidden_dim, num_layers, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WgXl_byuIPT"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQLGPbnCxJ0X"
      },
      "outputs": [],
      "source": [
        "def get_gradient_norms(model):\n",
        "    total_norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcBPrLN5V_5b"
      },
      "outputs": [],
      "source": [
        "trainingEpoch_loss = []\n",
        "validationEpoch_loss = []\n",
        "gradient_norms = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    step_loss = []\n",
        "    rnn_model.train()\n",
        "    for idx, train_inputs in enumerate(train_dataloader):\n",
        "        train_text, train_poses = train_inputs\n",
        "        train_text = train_text.to(device)\n",
        "        train_poses = train_poses.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = rnn_model(train_text)\n",
        "        training_loss = criterion(outputs, train_poses)\n",
        "        training_loss.backward()\n",
        "\n",
        "        grad_norm = get_gradient_norms(rnn_model)\n",
        "        gradient_norms.append(grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        step_loss.append(training_loss.item())\n",
        "\n",
        "        if (idx+1) % 1 == 0:\n",
        "          #for poses in outputs:\n",
        "          #  plot_openpose(poses.cpu().detach().numpy().reshape(5, 36))\n",
        "          print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')\n",
        "    trainingEpoch_loss.append(np.array(step_loss).mean())\n",
        "\n",
        "    #rnn_model.eval()\n",
        "    #for idx, val_inputs in enumerate(val_dataloader):\n",
        "    #  validationStep_loss = []\n",
        "    #  val_text, val_poses = val_inputs\n",
        "    #  val_text = val_text.to(device)\n",
        "    #  val_poses = val_poses.to(device)\n",
        "    #  outputs = rnn_model(val_text)\n",
        "    #  val_loss = criterion(outputs, val_poses)\n",
        "    #  validationStep_loss.append(val_loss.item())\n",
        "    #validationEpoch_loss.append(np.array(validationStep_loss).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v7_OLwOxHFt"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gradient_norms)\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Gradient norm')\n",
        "plt.title('Gradient Norms during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_v9sVMsuOL2"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(trainingEpoch_loss, label='train_loss')\n",
        "plt.plot(validationEpoch_loss,label='val_loss')\n",
        "plt.legend()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLHYNZG8EyZl"
      },
      "outputs": [],
      "source": [
        "#Save model to save weight folder\n",
        "model_save_name = 'rnn_numlayers=1_stack5_noval.pt'\n",
        "path = F\"/content/drive/MyDrive/Save Weight/{model_save_name}\"\n",
        "torch.save(rnn_model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4A8-4qFFlNN"
      },
      "outputs": [],
      "source": [
        "#Load saved model\n",
        "rnn_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Save Weight/rnn_numlayers=3_stack5_noval.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VtDLcDkqV_L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "rnn_model.eval()\n",
        "mae = []\n",
        "MAELoss = nn.L1Loss()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    outputs = rnn_model(batch[0].to(device))\n",
        "    test_loss = MAELoss(outputs, batch[1].to(device))\n",
        "    mae.append(test_loss)\n",
        "\n",
        "print(\"MAE Mean: \", np.array(mae).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLAKfCfhuT0M"
      },
      "outputs": [],
      "source": [
        "text = \"walk forward\"\n",
        "text2 = \"run sprint\"\n",
        "processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float).to(device)\n",
        "processed_text2 = torch.tensor(sentence_model.encode(text2), dtype=torch.float).to(device)\n",
        "output_poses = rnn_model(processed_text.unsqueeze(0))\n",
        "output_poses2 = rnn_model(processed_text2.unsqueeze(0))\n",
        "#print(output_poses.shape)\n",
        "print(output_poses[0] == output_poses2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rykw4P92uUtV"
      },
      "outputs": [],
      "source": [
        "keypoints_data = output_poses[0].tolist()\n",
        "people = []\n",
        "count = 1\n",
        "for i, xy in enumerate(keypoints_data):\n",
        "  if (i+1) % 36 == 0:\n",
        "    people.append(keypoints_data[i+1-36 : 36*count])\n",
        "    count += 1\n",
        "\n",
        "print(len(people))\n",
        "for person in people:\n",
        "  print(person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7swoTIxVE10"
      },
      "outputs": [],
      "source": [
        "plot_openpose(people)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mobcuptNeNaM"
      },
      "source": [
        "# **LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwvwKLa-eP3d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, output_dim):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.lstm5 = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.o = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        o_n1, (h_n1, c_n1) = self.lstm1(embedding)\n",
        "        o_n2, (h_n2, c_n2) = self.lstm2(o_n1, (h_n1, c_n1))\n",
        "        o_n3, (h_n3, c_n3) = self.lstm3(o_n2, (h_n2, c_n2))\n",
        "        o_n4, (h_n4, c_n4) = self.lstm4(o_n3, (h_n3, c_n3))\n",
        "        o_n5, (h_n5, c_n5) = self.lstm5(o_n4, (h_n4, c_n4))\n",
        "        output = self.o(o_n5)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u1_oZIqeVT-"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 384\n",
        "hidden_dim = 512\n",
        "num_layers = 1\n",
        "output_dim = 180\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lpOHxmJeWAc"
      },
      "outputs": [],
      "source": [
        "LSTM_model = LSTM(embedding_dim, hidden_dim, num_layers, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY8IYxtte_-l"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(LSTM_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JtT-Bjo1Ep5"
      },
      "outputs": [],
      "source": [
        "def get_gradient_norms(model):\n",
        "    total_norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u7Jq7hefDP0"
      },
      "outputs": [],
      "source": [
        "trainingEpoch_loss = []\n",
        "validationEpoch_loss = []\n",
        "gradient_norms = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    step_loss = []\n",
        "    LSTM_model.train()\n",
        "    for idx, train_inputs in enumerate(train_dataloader):\n",
        "        train_text, train_poses = train_inputs\n",
        "        train_text = train_text.to(device)\n",
        "        train_poses = train_poses.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = LSTM_model(train_text)\n",
        "        training_loss = criterion(outputs, train_poses)\n",
        "        training_loss.backward()\n",
        "\n",
        "        grad_norm = get_gradient_norms(LSTM_model)\n",
        "        gradient_norms.append(grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        step_loss.append(training_loss.item())\n",
        "\n",
        "        if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')\n",
        "    trainingEpoch_loss.append(np.array(step_loss).mean())\n",
        "\n",
        "    #LSTM_model.eval()\n",
        "    #for idx, val_inputs in enumerate(val_dataloader):\n",
        "    #  validationStep_loss = []\n",
        "    #  outputs = LSTM_model(val_inputs[0])\n",
        "    #  val_loss = criterion(outputs, val_inputs[1])\n",
        "    #  validationStep_loss.append(val_loss.item())\n",
        "    #validationEpoch_loss.append(np.array(validationStep_loss).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGRlt5Tn1G0j"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gradient_norms)\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Gradient norm')\n",
        "plt.title('Gradient Norms during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjbXuV6ufFpE"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(trainingEpoch_loss, label='train_loss')\n",
        "plt.plot(validationEpoch_loss,label='val_loss')\n",
        "plt.legend()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKu3kmHd0_gJ"
      },
      "outputs": [],
      "source": [
        "#Save model to save weight folder\n",
        "model_save_name = 'lstm_numlayers=1_stack5_noval.pt'\n",
        "path = F\"/content/drive/MyDrive/Save Weight/{model_save_name}\"\n",
        "torch.save(LSTM_model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXBq3CwO1Bxz"
      },
      "outputs": [],
      "source": [
        "#Load saved model\n",
        "LSTM_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Save Weight/lstm_numlayers=1_stack5_noval.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiAzE-KOyiEq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "LSTM_model.eval()\n",
        "mae = []\n",
        "MAELoss = nn.L1Loss()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    outputs = LSTM_model(batch[0].to(device))\n",
        "    test_loss = MAELoss(outputs, batch[1].to(device))\n",
        "    mae.append(test_loss)\n",
        "\n",
        "print(\"MAE Mean: \", np.array(mae).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5Sain7dfMc-"
      },
      "outputs": [],
      "source": [
        "text = \"a man running sprint\"\n",
        "processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)\n",
        "output_poses = LSTM_model(processed_text.unsqueeze(0))\n",
        "print(output_poses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-l5jS6LfOsR"
      },
      "outputs": [],
      "source": [
        "plot_openpose(output_poses.cpu().detach().numpy().reshape(5, 36))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oop7Ex0A00FJ"
      },
      "outputs": [],
      "source": [
        "people = output_poses.cpu().detach().numpy().reshape(5, 18, 2).tolist()\n",
        "\n",
        "newPeople = []\n",
        "for person in people:\n",
        "  newPerson = []\n",
        "  for keypoints in person:\n",
        "    newPerson.append([keypoints[0], keypoints[1], 1])\n",
        "  newPeople.append(newPerson)\n",
        "\n",
        "print(np.array(newPeople).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs6XtgL0zdc4"
      },
      "outputs": [],
      "source": [
        "def write_openpose_json(data, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        json.dump({ 'people': data, 'animals': [], 'canvas_width': 900, 'canvas_height': 300 }, f, indent=4)\n",
        "\n",
        "data = np.array(newPeople).reshape(5, 54).tolist()\n",
        "formatted_data = []\n",
        "for person in data:\n",
        "  formatted_data.append({ \"pose_keypoints_2d\": person })\n",
        "file_path = 'lstm_generated_poses_' + text + '.json'\n",
        "write_openpose_json(formatted_data, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmkesMclt4vB"
      },
      "source": [
        "# **CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQaFAJ_rt_id"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 48, 256)\n",
        "        self.fc2 = nn.Linear(256, output_dim)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 64 * 48)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-jS0ls2wveZ"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 384\n",
        "output_dim = 180\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNJnPX8KCWK9"
      },
      "outputs": [],
      "source": [
        "CNN_model = CNNModel(output_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMACBEhDuWEK"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(CNN_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_5nM6OHuKfy"
      },
      "outputs": [],
      "source": [
        "def get_gradient_norms(model):\n",
        "    total_norm = 0.0\n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    return total_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QD3jwCUc7OX"
      },
      "outputs": [],
      "source": [
        "trainingEpoch_loss = []\n",
        "validationEpoch_loss = []\n",
        "gradient_norms = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    step_loss = []\n",
        "    CNN_model.train()\n",
        "    for idx, train_inputs in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = CNN_model(train_inputs[0].unsqueeze(0).to(device))\n",
        "        training_loss = criterion(outputs, train_inputs[1].to(device))\n",
        "        training_loss.backward()\n",
        "\n",
        "        grad_norm = get_gradient_norms(CNN_model)\n",
        "        gradient_norms.append(grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        step_loss.append(training_loss.item())\n",
        "\n",
        "        if (idx+1) % 1 == 0: print (f'Epoch [{epoch+1}/{num_epochs}], Step [{idx+1}/{len(train_dataloader)}], Loss: {training_loss.item():.4f}')\n",
        "    trainingEpoch_loss.append(np.array(step_loss).mean())\n",
        "\n",
        "    #CNN_model.eval()\n",
        "    #for idx, val_inputs in enumerate(val_dataloader):\n",
        "    #  validationStep_loss = []\n",
        "    #  outputs = CNN_model(val_inputs[0].unsqueeze(0).to(device))\n",
        "    #  val_loss = criterion(outputs, val_inputs[1].to(device))\n",
        "    #  validationStep_loss.append(val_loss.item())\n",
        "    #validationEpoch_loss.append(np.array(validationStep_loss).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7LwBoVquWeS"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(gradient_norms)\n",
        "plt.xlabel('Batch number')\n",
        "plt.ylabel('Gradient norm')\n",
        "plt.title('Gradient Norms during Training')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhYOnEjXdH2k"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(trainingEpoch_loss, label='train_loss')\n",
        "plt.plot(validationEpoch_loss,label='val_loss')\n",
        "plt.legend()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAEz60PAutj1"
      },
      "outputs": [],
      "source": [
        "#Save model to save weight folder\n",
        "model_save_name = 'cnn_3layers_noval.pt'\n",
        "path = F\"/content/drive/MyDrive/Save Weight/{model_save_name}\"\n",
        "torch.save(CNN_model.state_dict(), path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywMl7ObvutwM"
      },
      "outputs": [],
      "source": [
        "#Load saved model\n",
        "CNN_model.load_state_dict(torch.load(\"/content/drive/MyDrive/Save Weight/cnn_3layers_noval.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rGcp8w7ytJg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "CNN_model.eval()\n",
        "mae = []\n",
        "MAELoss = nn.L1Loss()\n",
        "with torch.no_grad():\n",
        "  for i, batch in enumerate(test_dataloader):\n",
        "    outputs = CNN_model(batch[0].to(device))\n",
        "    test_loss = MAELoss(outputs, batch[1].to(device))\n",
        "    mae.append(test_loss)\n",
        "\n",
        "print(\"MAE Mean: \", np.array(mae).mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1vjYGC9u9Sn"
      },
      "outputs": [],
      "source": [
        "text = \"dead front\"\n",
        "processed_text = torch.tensor(sentence_model.encode(text), dtype=torch.float)\n",
        "output_poses = CNN_model(processed_text.unsqueeze(0).to(device))\n",
        "print(output_poses.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kya1NEnfvSi4"
      },
      "outputs": [],
      "source": [
        "keypoints_data = output_poses[0].tolist()\n",
        "people = []\n",
        "count = 1\n",
        "for i, xy in enumerate(keypoints_data):\n",
        "  if (i+1) % 36 == 0:\n",
        "    people.append(keypoints_data[i+1-36 : 36*count])\n",
        "    count += 1\n",
        "\n",
        "print(len(people))\n",
        "for person in people:\n",
        "  print(person)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL_D59LEvUYv"
      },
      "outputs": [],
      "source": [
        "plot_openpose(people)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iVvKApcy7uRv",
        "BlHd1jITYn6B",
        "zpxL8fS48f6x",
        "zYXcr7wBsboh",
        "ScdFNKHsYemp",
        "rtbtpbUVtvW8",
        "mobcuptNeNaM",
        "gmkesMclt4vB",
        "cyf51icbHdv3"
      ],
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
